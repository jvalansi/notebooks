{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "wanted-tampa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-3.8.3-cp36-cp36m-manylinux1_x86_64.whl (24.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.2 MB 7.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from gensim) (1.4.1)\n",
      "Requirement already satisfied: six>=1.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from gensim) (1.14.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from gensim) (1.18.1)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-4.2.0.tar.gz (119 kB)\n",
      "\u001b[K     |████████████████████████████████| 119 kB 67.7 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for smart-open: filename=smart_open-4.2.0-py3-none-any.whl size=109630 sha256=0a92972153a697c98809db78cb5e723469e7cf128ae9bcfe75add14d2f902366\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/05/12/87/d479d6a8f92130cd8b27e331cc433bb28dda9c20e57f0b1ab2\n",
      "Successfully built smart-open\n",
      "Installing collected packages: smart-open, gensim\n",
      "Successfully installed gensim-3.8.3 smart-open-4.2.0\n"
     ]
    }
   ],
   "source": [
    "! pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "supreme-bidder",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "voluntary-politics",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Write a function to perform the pre processing steps on the entire dataset\n",
    "'''\n",
    "stemmer = SnowballStemmer('english')\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text))\n",
    "\n",
    "# Tokenize and lemmatize\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "warming-december",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "offshore-continent",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/Thoughts_en/Untitled Document (10).cae2be5dc5b8d3d0a5bb898e4b99a2f9ed091bf610d814b73f08c057e4eff4a0.docx') as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "contained-hours",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = preprocess(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "potential-bleeding",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f912e857978e4f68b0a6d350fa65abb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=438.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dirname = 'data/Thoughts_en/'\n",
    "processed_docs = []\n",
    "for fname in tqdm(os.listdir(dirname)):\n",
    "    with open(dirname+'/'+fname) as f:\n",
    "        data = f.read()\n",
    "    tokens = preprocess(data)\n",
    "    processed_docs.append(tokens)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "pregnant-methodology",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "worth-gospel",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "indoor-notion",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model =  gensim.models.LdaMulticore(bow_corpus, \n",
    "                                   num_topics = 8, \n",
    "                                   id2word = dictionary,                                    \n",
    "                                   passes = 10,\n",
    "                                   workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "crazy-shareware",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "# Create a corpus from a list of texts\n",
    "common_dictionary = Dictionary(common_texts)\n",
    "common_corpus = [common_dictionary.doc2bow(text) for text in common_texts]\n",
    "\n",
    "# Train the model on the corpus.\n",
    "lda = LdaModel(common_corpus, num_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "brutal-words",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.020*\"year\" + 0.018*\"feel\" + 0.011*\"need\" + 0.010*\"million\" + 0.010*\"want\" + 0.009*\"negat\" + 0.009*\"work\" + 0.007*\"mayb\" + 0.007*\"know\" + 0.006*\"incom\"'),\n",
       " (1,\n",
       "  '0.015*\"like\" + 0.009*\"famili\" + 0.008*\"chanc\" + 0.007*\"year\" + 0.007*\"work\" + 0.007*\"write\" + 0.007*\"mayb\" + 0.006*\"invest\" + 0.006*\"asana\" + 0.006*\"hour\"'),\n",
       " (2,\n",
       "  '0.028*\"need\" + 0.018*\"year\" + 0.012*\"like\" + 0.012*\"write\" + 0.011*\"month\" + 0.009*\"mayb\" + 0.009*\"work\" + 0.009*\"apart\" + 0.009*\"health\" + 0.009*\"know\"'),\n",
       " (3,\n",
       "  '0.021*\"happi\" + 0.021*\"want\" + 0.015*\"think\" + 0.012*\"import\" + 0.011*\"like\" + 0.010*\"time\" + 0.010*\"know\" + 0.008*\"thing\" + 0.008*\"mayb\" + 0.007*\"need\"'),\n",
       " (4,\n",
       "  '0.020*\"feel\" + 0.019*\"good\" + 0.014*\"like\" + 0.012*\"quot\" + 0.011*\"want\" + 0.009*\"mayb\" + 0.008*\"time\" + 0.008*\"concept\" + 0.008*\"work\" + 0.007*\"know\"'),\n",
       " (5,\n",
       "  '0.015*\"need\" + 0.013*\"hour\" + 0.012*\"want\" + 0.010*\"year\" + 0.008*\"word\" + 0.008*\"time\" + 0.008*\"work\" + 0.007*\"mean\" + 0.007*\"chanc\" + 0.007*\"like\"'),\n",
       " (6,\n",
       "  '0.036*\"know\" + 0.030*\"mayb\" + 0.026*\"good\" + 0.021*\"work\" + 0.020*\"want\" + 0.019*\"need\" + 0.013*\"week\" + 0.012*\"like\" + 0.011*\"year\" + 0.011*\"time\"'),\n",
       " (7,\n",
       "  '0.011*\"know\" + 0.011*\"probabl\" + 0.010*\"mayb\" + 0.009*\"like\" + 0.009*\"think\" + 0.008*\"choic\" + 0.007*\"good\" + 0.006*\"cultur\" + 0.005*\"choos\" + 0.005*\"need\"')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modified-privilege",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
