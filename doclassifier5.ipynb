{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "from collections import Counter\n",
    "# import fasttext.util\n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "# fasttext.util.download_model('en', if_exists='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ! pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_df = pd.read_csv('data/Thoughts_en.doc.tsv', sep='\\t')\n",
    "doc_df = pd.read_csv('data/subs.actions.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_df = doc_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "msk = np.random.rand(len(doc_df)) < .95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc_df = doc_df[msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc_df = doc_df[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_doc_df.to_csv('data/thoughts/Thoughts_en.doc.train.tsv', header=['text','label'], index=False, sep='\\t')\n",
    "train_doc_df.to_csv('data/subs.actions.train.tsv', header=['text','label'], index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_doc_df.to_csv('data/thoughts/Thoughts_en.doc.test.tsv', header=['text','label'], index=False, sep='\\t')\n",
    "test_doc_df.to_csv('data/subs.actions.test.tsv', header=['text','label'], index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vec_df = pd.read_csv('data/thoughts/Thoughts_en.vec.tsv', sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_vec_df = vec_df[msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_vec_df = vec_df[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_vec_df.to_csv('data/Thoughts_en.vec.train.tsv', index=False, header=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_vec_df.to_csv('data/Thoughts_en.vec.test.tsv', index=False, header=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_vec_df['label'] = train_doc_df['is_action']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_vec_df['label'] = test_doc_df['is_action']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_vec_df.to_csv('data/Thoughts_en.vec.train.labeled.tsv', index=False, sep='\\t')\n",
    "# test_vec_df.to_csv('data/Thoughts_en.vec.test.labeled.tsv', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-abee3eae27b5665e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-abee3eae27b5665e/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8ab2b4d5e63404d8f1b7cccc5685e98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01130ebf06d04a528a959d29241d8239",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-abee3eae27b5665e/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffe3f736912f49489963b618b6c64eef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# dataset = load_dataset('csv', data_files={'train': 'data/thoughts/Thoughts_en.doc.train.tsv',\n",
    "#                                               'test': 'data/thoughts/Thoughts_en.doc.test.tsv'}, delimiter='\\t')\n",
    "dataset = load_dataset('csv', data_files={'train': 'data/subs.actions.train.tsv',\n",
    "                                              'test': 'data/subs.actions.test.tsv'}, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 176727\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 9305\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc4752b39da7421fb8c51fed1bf70dd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/177 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f929d313b2d4b898eaca9c7e8cd63c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=20)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(100)) \n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(100)) \n",
    "full_train_dataset = tokenized_datasets[\"train\"]\n",
    "full_eval_dataset = tokenized_datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\"subs_trainer\", evaluation_strategy=\"epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Trainer(\n",
    "#     model=model, args=training_args, train_dataset=small_train_dataset, eval_dataset=small_eval_dataset\n",
    "# )\n",
    "trainer = Trainer(\n",
    "    model=model, args=training_args, train_dataset=full_train_dataset, eval_dataset=full_eval_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"subs_trainer/checkpoint-66000/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "    outputs = model(**inputs, labels=labels)\n",
    "    return torch.nn.Softmax(dim=1)(outputs.logits).tolist()[0][1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999179840087891"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"Alice, you need to finish my cereal by tomorrow\")\n",
    "# inputs = tokenizer(\"Fuck you\", return_tensors=\"pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs_df = pd.read_csv('data/subs.actions.test.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:NumExpr defaulting to 4 threads.\n"
     ]
    }
   ],
   "source": [
    "subs_df = subs_df[subs_df['label']==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3f24007d5db44038f660b4aac7509ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4636 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "subs_df['prediction'] = subs_df['text'].progress_apply(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs_df = subs_df.sort_values('prediction', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Downloading the data would cause death to the data carrier.'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subs_df.iloc[1,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use POS to identify Action Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from string import punctuation\n",
    "import nltk\n",
    "import nltk.data\n",
    "from collections import Counter\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_df = pd.read_csv('data/thoughts/Thoughts_en.doc.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_df = pd.read_csv('data/subs.actions.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_df = doc_df[doc_df['label']==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_df = pd.read_csv('data/ami.sentence.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verbs = []\n",
    "def is_action(text):\n",
    "    if type(text)!=str:\n",
    "        return False\n",
    "    if \"?\" in text:\n",
    "        return False\n",
    "#     if 'need' in text:\n",
    "#         return True\n",
    "    text = re.sub(',', '.', text)\n",
    "#     split = tokenizer.tokenize(text)\n",
    "    sents = nlp(text)\n",
    "    for sent in sents.sents:\n",
    "        if not sent:\n",
    "            continue\n",
    "        for word in sent:\n",
    "            if word.dep_=='neg':\n",
    "                return False\n",
    "            if word.dep_=='ROOT' and word.pos_=='VERB':\n",
    "                try:\n",
    "                    morph = word.morph.to_dict()\n",
    "                    if morph['VerbForm']=='Inf':\n",
    "                        return True\n",
    "                except KeyError:\n",
    "                    continue\n",
    "#         if sen[0].pos_=='VERB':\n",
    "#             return True\n",
    "#     #     if '!' in x and any(word.pos_=='VERB' for word in sen): # Doesn't work\n",
    "#     #         return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f99e1b54215f49c68972a53e65990973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83436 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# doc_df['is_action'] = doc_df['doc'].progress_apply(is_action)\n",
    "doc_df['is_action'] = doc_df['text'].progress_apply(is_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "names        [hello]\n",
       "locations    [paris]\n",
       "dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series({'names':['hello'], 'locations':['paris']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sent(sent):\n",
    "    parse = {'dates':[], 'locations':[], 'names':[]}\n",
    "    try:\n",
    "        sentence = nlp(sent)\n",
    "    except ValueError:\n",
    "        return pd.Series(parse)\n",
    "    ents = sentence.ents\n",
    "    for token in sentence:\n",
    "        if token.ent_type_ == \"DATE\":\n",
    "            parse['dates'] += [token]\n",
    "        if token.ent_type_ == \"GPE\":\n",
    "            parse['locations'] += [token]\n",
    "        if token.pos_!='PROPN':\n",
    "            continue\n",
    "        if token.ent_type_ in [\"DATE\", \"GPE\"]:\n",
    "            continue\n",
    "        parse['names']+= [token]\n",
    "    return pd.Series(parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf4671fdffb84d8ead9adf5a35b923fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83436 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parse_df = doc_df.loc[:, 'text'].progress_apply(parse_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_df[['dates','locations','names']] = parse_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_df= doc_df.drop('parse', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_names(sent):\n",
    "    try:\n",
    "        sentence = nlp(sent)\n",
    "    except ValueError:\n",
    "        return []\n",
    "    names = []\n",
    "    ents = sentence.ents\n",
    "    for token in sentence:\n",
    "        if token.pos_!='PROPN':\n",
    "            continue\n",
    "        if token.ent_type_ in [\"DATE\", \"GPE\"]:\n",
    "            continue\n",
    "        names.append(token)\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_df['names'] = doc_df['text'].progress_apply(get_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ents(sent, type_):\n",
    "    try:\n",
    "        sentence = nlp(sent)\n",
    "    except ValueError:\n",
    "        return []\n",
    "    ents = []\n",
    "    for entity in sentence.ents:\n",
    "        if entity.label_==type_:\n",
    "            ents.append(entity)\n",
    "    return ents\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_dates = partial(get_ents, type_=\"DATE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_locations = partial(get_ents, type_=\"GPE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_df['dates'] = doc_df['text'].progress_apply(get_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_df['locations'] = doc_df['text'].progress_apply(get_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_df.to_csv('data/Thoughts_en.doc.tsv', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_df.to_csv('data/subs.action.name.date.doc.tsv', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_df.to_csv('data/ami.sentence.labelled.tsv', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_df = doc_df[ doc_df['is_action'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actions_df.to_csv('data/subs.action.name.date.doc.tsv', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_df.to_csv('data/ami.action.tsv', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>starttime</th>\n",
       "      <th>text</th>\n",
       "      <th>fname</th>\n",
       "      <th>is_action</th>\n",
       "      <th>dates</th>\n",
       "      <th>locations</th>\n",
       "      <th>names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64.47</td>\n",
       "      <td>Well I'll uh start just with another presentat...</td>\n",
       "      <td>TS3009b.A.words.xml</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>99.75</td>\n",
       "      <td>Well um in we'll uh just have a look at the at...</td>\n",
       "      <td>TS3009b.A.words.xml</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[dec]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>110.82</td>\n",
       "      <td>Uh then we'll uh look at uh the three uh prese...</td>\n",
       "      <td>TS3009b.A.words.xml</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>132.31</td>\n",
       "      <td>Oh I've received a mail with uh some additiona...</td>\n",
       "      <td>TS3009b.A.words.xml</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>159.54</td>\n",
       "      <td>And then uh we can discuss uh some more closel...</td>\n",
       "      <td>TS3009b.A.words.xml</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83413</th>\n",
       "      <td>2301.58</td>\n",
       "      <td>Break it , I don't get</td>\n",
       "      <td>TS3007b.B.words.xml</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83419</th>\n",
       "      <td>2406.55</td>\n",
       "      <td>'Kay , save it as an image on the res</td>\n",
       "      <td>TS3007b.B.words.xml</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Kay]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83420</th>\n",
       "      <td>2420.00</td>\n",
       "      <td>No , and use an image if possible .</td>\n",
       "      <td>TS3007b.B.words.xml</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83430</th>\n",
       "      <td>2471.98</td>\n",
       "      <td>The questionnaire , fill in uh we fill out d a...</td>\n",
       "      <td>TS3007b.B.words.xml</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83433</th>\n",
       "      <td>2488.87</td>\n",
       "      <td>We can leave the P_C_ on I think , yeah and re...</td>\n",
       "      <td>TS3007b.B.words.xml</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[P_C]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9680 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       starttime                                               text  \\\n",
       "0          64.47  Well I'll uh start just with another presentat...   \n",
       "6          99.75  Well um in we'll uh just have a look at the at...   \n",
       "7         110.82  Uh then we'll uh look at uh the three uh prese...   \n",
       "9         132.31  Oh I've received a mail with uh some additiona...   \n",
       "11        159.54  And then uh we can discuss uh some more closel...   \n",
       "...          ...                                                ...   \n",
       "83413    2301.58                            Break it , I don't get    \n",
       "83419    2406.55             'Kay , save it as an image on the res    \n",
       "83420    2420.00               No , and use an image if possible .    \n",
       "83430    2471.98  The questionnaire , fill in uh we fill out d a...   \n",
       "83433    2488.87  We can leave the P_C_ on I think , yeah and re...   \n",
       "\n",
       "                     fname  is_action dates locations  names  \n",
       "0      TS3009b.A.words.xml       True    []        []     []  \n",
       "6      TS3009b.A.words.xml       True    []        []  [dec]  \n",
       "7      TS3009b.A.words.xml       True    []        []     []  \n",
       "9      TS3009b.A.words.xml       True    []        []     []  \n",
       "11     TS3009b.A.words.xml       True    []        []     []  \n",
       "...                    ...        ...   ...       ...    ...  \n",
       "83413  TS3007b.B.words.xml       True    []        []     []  \n",
       "83419  TS3007b.B.words.xml       True    []        []  [Kay]  \n",
       "83420  TS3007b.B.words.xml       True    []        []     []  \n",
       "83430  TS3007b.B.words.xml       True    []        []     []  \n",
       "83433  TS3007b.B.words.xml       True    []        []  [P_C]  \n",
       "\n",
       "[9680 rows x 7 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate synthetic examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time\n",
    "# time template\n",
    "preposition = [\"by\",\"for\",\"until\", 'till', '']\n",
    "timeframe = [\"end of\", \"next\", \"this\", \"\"]\n",
    "day = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"today\", \"tomorrow\", \"week\", \"the 20th\", \"the first of July\", \"\"]\n",
    "time = [\"3pm\", \"afternoon\", \"morning\", \"at 5pm\", \"noon\", \"\"]\n",
    "\n",
    "def gen_time():\n",
    "    prep = random.choice(preposition)\n",
    "    tf = random.choice(timeframe)\n",
    "    d = random.choice(day)\n",
    "    t = random.choice(time)\n",
    "\n",
    "    return \" \".join([prep, tf, d, t])\n",
    "\n",
    "# \n",
    "# name\n",
    "# action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_dfs = pd.read_html('https://www.ssa.gov/oact/babynames/decades/century.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_df = name_dfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = list(name_df['Males'][\"Name\"])+list(name_df['Females'][\"Name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# action\n",
    "asks = [\"can you please\", \"can you\", \"please\", \"\", \"you need to\"]\n",
    "actions = [\"read\", \"write\", \"fix\", \"go over\", \"correct\", \"submit\", \"discuss\", \"run\", \"execute\", \"improve\", \"purchase\", \"review\"]\n",
    "adjectives = [\"the last\", \"the\", \"the new\", \"this\", \"that\", \"the old\"]\n",
    "subjects = [\"report\", \"code\", \"account\", \"review\", \"data\", \"record\", \"description\"]\n",
    "\n",
    "def gen_action():\n",
    "    a = random.choice(asks)\n",
    "    act = random.choice(actions)\n",
    "    adj = random.choice(adjectives)\n",
    "    subj = random.choice(subjects)\n",
    "\n",
    "    return \" \".join([a, act, adj, subj])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = [\n",
    "    \"{name}, {action} {time}\",\n",
    "    \"{name}, {time} {action}\",\n",
    "    \"{action} {time}, {name}\",\n",
    "    \"{time} {action}, {name}\"\n",
    "]\n",
    "\n",
    "def gen_action_item():\n",
    "    name = random.choice(names)\n",
    "    action = gen_action()\n",
    "    time = gen_time()\n",
    "    temp = random.choice(templates)\n",
    "    \n",
    "    \n",
    "\n",
    "    action_item = temp.format(name=name, action=action, time=time)\n",
    "    return action_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' end of Thursday morning you need to improve this data, Brandon'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_action_item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenSubtitles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset open_subtitles (/root/.cache/huggingface/datasets/open_subtitles/en-hi/2018.0.0/c1ec973ca4b6e588740d8f167cc0e24ea3f626e70bc7ffe467e944730500e198)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4435c3ea6af54d5cb48dfbd76fba3acb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "open_sub = load_dataset('open_subtitles', 'en-hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_sub = [x['en'] for x in open_sub['train']['translation']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_items = [gen_action_item() for _ in range(len(en_sub))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs_df = pd.DataFrame(list(zip(en_sub, [False]*len(en_sub))) + list(zip(action_items,[True]*len(action_items))), columns=['text', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>THE BICYCLE THIEF</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ricci?</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Is Ricci there?</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Are you deaf? Come on!</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Get a move on.</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     text  label\n",
       "0       THE BICYCLE THIEF  False\n",
       "1                  Ricci?  False\n",
       "2         Is Ricci there?  False\n",
       "3  Are you deaf? Come on!  False\n",
       "4          Get a move on.  False"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>186027</th>\n",
       "      <td>please submit this description  next the 20th ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186028</th>\n",
       "      <td>you need to write the old review by next Tuesd...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186029</th>\n",
       "      <td>Patrick, till this the 20th noon can you submi...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186030</th>\n",
       "      <td>can you go over the new account till this Frid...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186031</th>\n",
       "      <td>Donna, until   afternoon please fix the last r...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  label\n",
       "186027  please submit this description  next the 20th ...   True\n",
       "186028  you need to write the old review by next Tuesd...   True\n",
       "186029  Patrick, till this the 20th noon can you submi...   True\n",
       "186030  can you go over the new account till this Frid...   True\n",
       "186031  Donna, until   afternoon please fix the last r...   True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subs_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs_df.to_csv('data/subs.actions.tsv', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_action(text):\n",
    "    sen = nlp(text)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subs_df[subs_df['text'].apply(lambda x: '!' in x, sen = nlp(x))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meeting minutes data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! wget -P data/ http://groups.inf.ed.ac.uk/ami/AMICorpusAnnotations/ami_public_manual_1.6.2.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir data/ami_public_manual_1.6.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! unzip data/ami_public_manual_1.6.2.zip -d data/ami_public_manual_1.6.2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! wget -P data/ https://webtransfer.vancouver.ca/OpenData/txt/CouncilMeetingMinutesArchival.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head data/ami_public_manual_1.6.2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences(fname):\n",
    "    tree = ET.parse('data/ami_public_manual_1.6.2/words/'+fname)\n",
    "    root = tree.getroot()\n",
    "    data = []\n",
    "    for node in root:\n",
    "        try:\n",
    "            data.append([node.text, node.attrib['starttime'], node.attrib['endtime']])\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "    if not data:\n",
    "        return []\n",
    "            \n",
    "    # # checking the word distance histogram to decide how to identify new sentences\n",
    "    # df = pd.DataFrame(data, columns=['text', 'starttime', 'endtime'])\n",
    "    # df['starttime'] = pd.to_numeric(df['starttime'])\n",
    "    # df['endtime'] = pd.to_numeric(df['endtime'])\n",
    "    # df['length'] = df['endtime'] - df['starttime']\n",
    "    # df['diff'] = df['endtime'].diff()\n",
    "    # df['dist'] = df['diff']-df['length']\n",
    "    # df['dist'].hist(bins=[1*i for i in range(10)])\n",
    "\n",
    "    sentences = []\n",
    "    prev,prev_s,prev_e = data[0]\n",
    "    sentence = prev+\" \" if prev else \"\"\n",
    "    sentence_s = prev_s\n",
    "    for word,s,e in data[1:]:\n",
    "        if not word:\n",
    "            continue\n",
    "        if float(s)-float(prev_e)>0.1:\n",
    "            sentences.append((sentence_s,sentence,fname))\n",
    "            sentence_s = s\n",
    "            sentence = \"\"\n",
    "        sentence += word+\" \"\n",
    "        prev,prev_s,prev_e = word,s,e\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b617e6f2d85f46dcb88f8eef59c33a8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/687 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentences = []\n",
    "for fname in tqdm(list(os.listdir('data/ami_public_manual_1.6.2/words/'))):\n",
    "    if not fname.endswith('.xml'):\n",
    "        continue\n",
    "    sentences += extract_sentences(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_df = pd.DataFrame(sentences, columns=['starttime', 'text', 'fname'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83436"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentence_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_df.to_csv('data/ami.sentence.tsv', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.8 Python 3.6 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/pytorch-1.8-gpu-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
