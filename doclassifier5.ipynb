{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "from collections import Counter\n",
    "# import fasttext.util\n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "# fasttext.util.download_model('en', if_exists='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ! pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_df = pd.read_csv('data/Thoughts_en.doc.tsv', sep='\\t')\n",
    "doc_df = pd.read_csv('data/subs.actions.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_df = doc_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "msk = np.random.rand(len(doc_df)) < .95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc_df = doc_df[msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc_df = doc_df[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_doc_df.to_csv('data/thoughts/Thoughts_en.doc.train.tsv', header=['text','label'], index=False, sep='\\t')\n",
    "train_doc_df.to_csv('data/subs.actions.train.tsv', header=['text','label'], index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_doc_df.to_csv('data/thoughts/Thoughts_en.doc.test.tsv', header=['text','label'], index=False, sep='\\t')\n",
    "test_doc_df.to_csv('data/subs.actions.test.tsv', header=['text','label'], index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vec_df = pd.read_csv('data/thoughts/Thoughts_en.vec.tsv', sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_vec_df = vec_df[msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_vec_df = vec_df[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_vec_df.to_csv('data/Thoughts_en.vec.train.tsv', index=False, header=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_vec_df.to_csv('data/Thoughts_en.vec.test.tsv', index=False, header=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_vec_df['label'] = train_doc_df['is_action']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_vec_df['label'] = test_doc_df['is_action']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_vec_df.to_csv('data/Thoughts_en.vec.train.labeled.tsv', index=False, sep='\\t')\n",
    "# test_vec_df.to_csv('data/Thoughts_en.vec.test.labeled.tsv', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-abee3eae27b5665e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-abee3eae27b5665e/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8ab2b4d5e63404d8f1b7cccc5685e98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01130ebf06d04a528a959d29241d8239",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-abee3eae27b5665e/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffe3f736912f49489963b618b6c64eef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# dataset = load_dataset('csv', data_files={'train': 'data/thoughts/Thoughts_en.doc.train.tsv',\n",
    "#                                               'test': 'data/thoughts/Thoughts_en.doc.test.tsv'}, delimiter='\\t')\n",
    "dataset = load_dataset('csv', data_files={'train': 'data/subs.actions.train.tsv',\n",
    "                                              'test': 'data/subs.actions.test.tsv'}, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 176727\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 9305\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc4752b39da7421fb8c51fed1bf70dd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/177 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f929d313b2d4b898eaca9c7e8cd63c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=20)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(100)) \n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(100)) \n",
    "full_train_dataset = tokenized_datasets[\"train\"]\n",
    "full_eval_dataset = tokenized_datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\"subs_trainer\", evaluation_strategy=\"epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Trainer(\n",
    "#     model=model, args=training_args, train_dataset=small_train_dataset, eval_dataset=small_eval_dataset\n",
    "# )\n",
    "trainer = Trainer(\n",
    "    model=model, args=training_args, train_dataset=full_train_dataset, eval_dataset=full_eval_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"subs_trainer/checkpoint-66000/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "    outputs = model(**inputs, labels=labels)\n",
    "    return torch.nn.Softmax(dim=1)(outputs.logits).tolist()[0][1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999179840087891"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"Alice, you need to finish my cereal by tomorrow\")\n",
    "# inputs = tokenizer(\"Fuck you\", return_tensors=\"pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs_df = pd.read_csv('data/subs.actions.test.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:NumExpr defaulting to 4 threads.\n"
     ]
    }
   ],
   "source": [
    "subs_df = subs_df[subs_df['label']==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3f24007d5db44038f660b4aac7509ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4636 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "subs_df['prediction'] = subs_df['text'].progress_apply(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs_df = subs_df.sort_values('prediction', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Downloading the data would cause death to the data carrier.'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subs_df.iloc[1,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use POS to identify Action Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from string import punctuation\n",
    "import nltk\n",
    "import nltk.data\n",
    "from collections import Counter\n",
    "from functools import partial\n",
    "\n",
    "# nltk.download()\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_df = pd.read_csv('data/thoughts/Thoughts_en.doc.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_df = pd.read_csv('data/subs.actions.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_df = doc_df[doc_df['label']==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_df = pd.read_csv('data/ami.sentence.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_action_verbs = ['let', 'see', 'say', 'like', 'forget']\n",
    "# check https://www.englishclub.com/grammar/verbs-continuous-meaning.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs = []\n",
    "def is_action(text):\n",
    "    if type(text)!=str:\n",
    "        return False\n",
    "    if \"?\" in text:\n",
    "        return False\n",
    "#     if 'need' in text:\n",
    "#         return True\n",
    "    text = re.sub(',', '.', text)\n",
    "#     split = tokenizer.tokenize(text)\n",
    "    sents = nlp(text)\n",
    "    for sent in sents.sents:\n",
    "        if not sent:\n",
    "            continue\n",
    "        for word in sent:\n",
    "            if word.dep_=='neg':\n",
    "                return False\n",
    "            if word.dep_=='ROOT' and word.pos_=='VERB':\n",
    "                if word.text.lower() in non_action_verbs:\n",
    "                    continue\n",
    "                try:\n",
    "                    morph = word.morph.to_dict()\n",
    "                    if morph['VerbForm']=='Inf':\n",
    "                        verbs.append(word)\n",
    "                        return word\n",
    "                except KeyError:\n",
    "                    continue\n",
    "#         if sen[0].pos_=='VERB':\n",
    "#             return True\n",
    "#     #     if '!' in x and any(word.pos_=='VERB' for word in sen): # Doesn't work\n",
    "#     #         return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26fdc81924484254aa1c3ae5ad3ea640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/194480 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# doc_df['is_action'] = doc_df['doc'].progress_apply(is_action)\n",
    "doc_df['is_action'] = doc_df['text'].progress_apply(is_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_words = [verb.text.lower() for verb in verbs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = Counter(verb_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnt.most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# issues:\n",
    "# Multi words\n",
    "# noise (single letter words)\n",
    "# Initials\n",
    "# names identified as locations\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sent(sent):\n",
    "    parse = {'dates':[], 'locations':[], 'names':[]}\n",
    "    try:\n",
    "        sentence = nlp(sent)\n",
    "    except ValueError:\n",
    "        return pd.Series(parse)\n",
    "    ents = sentence.ents\n",
    "    for ent in ents:\n",
    "        if ent.label_==\"DATE\":\n",
    "            parse['dates'] += [ent]\n",
    "        if ent.label_==\"GPE\":\n",
    "            parse['locations'] += [ent]\n",
    "    for token in sentence:\n",
    "        if len(token.text)==1: #reduce noise\n",
    "            continue\n",
    "        if \".\" in token.text: # initials\n",
    "            continue\n",
    "        if token.text[0].islower(): # non name\n",
    "            continue\n",
    "        if token.ent_type_ == \"DATE\":\n",
    "            continue\n",
    "        if token.ent_type_ == \"GPE\":\n",
    "            continue\n",
    "        if token.pos_!='PROPN':\n",
    "            continue\n",
    "        if token.ent_type_ in [\"DATE\", \"GPE\"]:\n",
    "            continue\n",
    "        parse['names']+= [token]\n",
    "    return pd.Series(parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41327806dcdd40d4857a039a7d3e9a12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/194480 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parse_df = doc_df.loc[:, 'text'].progress_apply(parse_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_df[['dates','locations','names']] = parse_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_df.to_csv('data/Thoughts_en.doc.tsv', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_df.to_csv('data/subs.action.name.date.doc.tsv', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_df.to_csv('data/ami.sentence.labelled.tsv', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(194480, 7)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_df = doc_df[ doc_df['is_action']!=False ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actions_df.to_csv('data/subs.action.name.date.doc.tsv', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_df.to_csv('data/ami.action.tsv', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11662, 7)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate synthetic examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time\n",
    "# time template\n",
    "preposition = [\"by\",\"for\",\"until\", 'till', '']\n",
    "timeframe = [\"end of\", \"next\", \"this\", \"\"]\n",
    "day = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"today\", \"tomorrow\", \"week\", \"the 20th\", \"the first of July\", \"\"]\n",
    "time = [\"3pm\", \"afternoon\", \"morning\", \"at 5pm\", \"noon\", \"\"]\n",
    "\n",
    "def gen_time():\n",
    "    prep = random.choice(preposition)\n",
    "    tf = random.choice(timeframe)\n",
    "    d = random.choice(day)\n",
    "    t = random.choice(time)\n",
    "\n",
    "    return \" \".join([prep, tf, d, t])\n",
    "\n",
    "# \n",
    "# name\n",
    "# action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_dfs = pd.read_html('https://www.ssa.gov/oact/babynames/decades/century.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_df = name_dfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = list(name_df['Males'][\"Name\"])+list(name_df['Females'][\"Name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# action\n",
    "asks = [\"can you please\", \"can you\", \"please\", \"\", \"you need to\"]\n",
    "actions = [\"read\", \"write\", \"fix\", \"go over\", \"correct\", \"submit\", \"discuss\", \"run\", \"execute\", \"improve\", \"purchase\", \"review\"]\n",
    "adjectives = [\"the last\", \"the\", \"the new\", \"this\", \"that\", \"the old\"]\n",
    "subjects = [\"report\", \"code\", \"account\", \"review\", \"data\", \"record\", \"description\"]\n",
    "\n",
    "def gen_action():\n",
    "    a = random.choice(asks)\n",
    "    act = random.choice(actions)\n",
    "    adj = random.choice(adjectives)\n",
    "    subj = random.choice(subjects)\n",
    "\n",
    "    return \" \".join([a, act, adj, subj])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = [\n",
    "    \"{name}, {action} {time}\",\n",
    "    \"{name}, {time} {action}\",\n",
    "    \"{action} {time}, {name}\",\n",
    "    \"{time} {action}, {name}\"\n",
    "]\n",
    "\n",
    "def gen_action_item():\n",
    "    name = random.choice(names)\n",
    "    action = gen_action()\n",
    "    time = gen_time()\n",
    "    temp = random.choice(templates)\n",
    "    \n",
    "    \n",
    "\n",
    "    action_item = temp.format(name=name, action=action, time=time)\n",
    "    return action_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' end of Thursday morning you need to improve this data, Brandon'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_action_item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenSubtitles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset open_subtitles (/root/.cache/huggingface/datasets/open_subtitles/en-hi/2018.0.0/c1ec973ca4b6e588740d8f167cc0e24ea3f626e70bc7ffe467e944730500e198)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4435c3ea6af54d5cb48dfbd76fba3acb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "open_sub = load_dataset('open_subtitles', 'en-hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_sub = [x['en'] for x in open_sub['train']['translation']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_items = [gen_action_item() for _ in range(len(en_sub))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs_df = pd.DataFrame(list(zip(en_sub, [False]*len(en_sub))) + list(zip(action_items,[True]*len(action_items))), columns=['text', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>THE BICYCLE THIEF</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ricci?</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Is Ricci there?</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Are you deaf? Come on!</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Get a move on.</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     text  label\n",
       "0       THE BICYCLE THIEF  False\n",
       "1                  Ricci?  False\n",
       "2         Is Ricci there?  False\n",
       "3  Are you deaf? Come on!  False\n",
       "4          Get a move on.  False"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>186027</th>\n",
       "      <td>please submit this description  next the 20th ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186028</th>\n",
       "      <td>you need to write the old review by next Tuesd...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186029</th>\n",
       "      <td>Patrick, till this the 20th noon can you submi...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186030</th>\n",
       "      <td>can you go over the new account till this Frid...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186031</th>\n",
       "      <td>Donna, until   afternoon please fix the last r...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  label\n",
       "186027  please submit this description  next the 20th ...   True\n",
       "186028  you need to write the old review by next Tuesd...   True\n",
       "186029  Patrick, till this the 20th noon can you submi...   True\n",
       "186030  can you go over the new account till this Frid...   True\n",
       "186031  Donna, until   afternoon please fix the last r...   True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subs_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs_df.to_csv('data/subs.actions.tsv', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_action(text):\n",
    "    sen = nlp(text)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subs_df[subs_df['text'].apply(lambda x: '!' in x, sen = nlp(x))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meeting minutes data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! wget -P data/ http://groups.inf.ed.ac.uk/ami/AMICorpusAnnotations/ami_public_manual_1.6.2.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir data/ami_public_manual_1.6.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! unzip data/ami_public_manual_1.6.2.zip -d data/ami_public_manual_1.6.2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! wget -P data/ https://webtransfer.vancouver.ca/OpenData/txt/CouncilMeetingMinutesArchival.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences(fname):\n",
    "    tree = ET.parse('data/ami_public_manual_1.6.2/words/'+fname)\n",
    "    root = tree.getroot()\n",
    "    data = []\n",
    "    for node in root:\n",
    "        try:\n",
    "            data.append([node.text, node.attrib['starttime'], node.attrib['endtime']])\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "    if not data:\n",
    "        return []\n",
    "            \n",
    "    # # checking the word distance histogram to decide how to identify new sentences\n",
    "    # df = pd.DataFrame(data, columns=['text', 'starttime', 'endtime'])\n",
    "    # df['starttime'] = pd.to_numeric(df['starttime'])\n",
    "    # df['endtime'] = pd.to_numeric(df['endtime'])\n",
    "    # df['length'] = df['endtime'] - df['starttime']\n",
    "    # df['diff'] = df['endtime'].diff()\n",
    "    # df['dist'] = df['diff']-df['length']\n",
    "    # df['dist'].hist(bins=[1*i for i in range(10)])\n",
    "\n",
    "    sentences = []\n",
    "    prev,prev_s,prev_e = data[0]\n",
    "    sentence = prev+\" \" if prev else \"\"\n",
    "    sentence_s = prev_s\n",
    "    for word,s,e in data[1:]:\n",
    "        if not word:\n",
    "            continue\n",
    "        word = re.sub('_','.',word)\n",
    "        if float(s)-float(prev_e)>0.1:\n",
    "            sentences.append((sentence_s,sentence,fname))\n",
    "            sentence_s = s\n",
    "            sentence = \"\"\n",
    "        if word in ['.','?','!']:\n",
    "            sentences.append((sentence_s,sentence + word,fname))\n",
    "            sentence_s = s\n",
    "            sentence = \"\"\n",
    "        else:\n",
    "            sentence += word + \" \"\n",
    "        prev,prev_s,prev_e = word,s,e\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a5225187c2d41189b96d85636ce8c7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/687 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentences = []\n",
    "for fname in tqdm(list(os.listdir('data/ami_public_manual_1.6.2/words/'))):\n",
    "    if not fname.endswith('.xml'):\n",
    "        continue\n",
    "    sentences += extract_sentences(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_df = pd.DataFrame(sentences, columns=['starttime', 'text', 'fname'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "194480"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentence_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_df.to_csv('data/ami.sentence.tsv', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.8 Python 3.6 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/pytorch-1.8-gpu-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
